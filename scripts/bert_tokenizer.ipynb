{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsuBAlr5iJas"
   },
   "source": [
    "# Colab specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5MaZRISly6a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uz3M46WQiHnO"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "414L3v_IiPR-"
   },
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUvEECwjj9PL"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgLIckzyj9PT"
   },
   "outputs": [],
   "source": [
    "# Jupyter is expected to be run from git repository root directory.\n",
    "# By default CategoryLearning is the directory name. Modify path in case of an error.\n",
    "# It's is recommended to use an absolute path in lib_path\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "# DEFINE PATH BELOW\n",
    "# lib_path = Path('/content/drive/My Drive/Colab/CategoryLearning')\n",
    "lib_path = Path.resolve(Path('../'))\n",
    "print(f'Current absolute path to Git root directory: {lib_path}')\n",
    "assert 'catlearn' in os.listdir(lib_path), 'It seems notebook is run not from the root directory. Modify lib_path.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2nBGlTJj9PX"
   },
   "outputs": [],
   "source": [
    "# Modify notebook environment path in case it was updated in the cell above\n",
    "os.chdir(lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEE1nJZGj9PX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from sklearn.decomposition import PCA\n",
    "from data.config import (raw_dataset_pat, preproc_pat)\n",
    "from data.utils import read_file, write_file\n",
    "from typing import Dict, List, Set, Union, Any\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ME_c0Z4WVNX"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7rei7dpj9PZ"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_MwG_Eej9PZ"
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', \n",
    "                                 output_hidden_states = True,\n",
    "                                 )\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBYsOWehmoEb"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJu9jaV8j9Pb"
   },
   "outputs": [],
   "source": [
    "def word2vec_bert(sentence: str):\n",
    "    \"\"\"\n",
    "    Return outputs of all 12 layers of BERT plus\n",
    "    output of the final FC layer.\n",
    "    All in the format [n_token, n_layer, emb_size],\n",
    "    where n_token is equal to n-2 tokens with added\n",
    "    zero position [CLS] and last position [SEP],\n",
    "    where n_layers is 13 (12 + 1),\n",
    "    where emb_size is 768.\n",
    "    Use token_embeddings[12] to get final output\n",
    "    \"\"\"\n",
    "    marked_sentence = '[CLS] ' + sentence + ' [SEP]'\n",
    "\n",
    "    tokenized = tokenizer.tokenize(marked_sentence)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    # for tup in zip(tokenized, indexed_tokens):\n",
    "        # print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "    indexed_tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    sentence_ids = [1] * len(tokenized)\n",
    "    sentence_ids_tensors = torch.tensor([sentence_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.forward(input_ids=indexed_tokens_tensor, attention_mask=sentence_ids_tensors)\n",
    "        # same as: outputs = model(input_ids=tokens_tensor, attention_mask=segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z89aSVw4j9Pj"
   },
   "source": [
    "Note: In average, words without separators [CLS], [SEP] are quite different in meaning from analogoues with separators\n",
    "Separators are needed for consistency of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ1R4Fy9j9Pl"
   },
   "source": [
    "## Read dataset and create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx7hqT5Mj9Pm"
   },
   "outputs": [],
   "source": [
    "path_dir_wordset = './Datasets/wn18rr/text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOlYshjfj9Pm"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(path_dir_wordset: str):\n",
    "    \"\"\"\n",
    "    Generate embedding wectors for each word.\n",
    "    path_dir_wordset: path to the file with a set of words\n",
    "    output word_embeddings: dictionary with a word as a key\n",
    "    and 768 dim vector for the mebedding\n",
    "    Note: for composend words with several embeddings per word,\n",
    "    will average all embeding vectors.\n",
    "    return: word_embeddings: dict {'str': Ndarray}\n",
    "    lengthes: dict {'str': int} -- number of embedding wectors per word/phrase\n",
    "    \"\"\"\n",
    "    word_set: List[str] = read_file(os.path.join(path_dir_wordset, preproc_pat['wordset']))\n",
    "    \n",
    "    word_embeddings: Dict = {}\n",
    "    lengthes = {}\n",
    "    for word in tqdm(word_set):\n",
    "        # split compound words on members\n",
    "        sentence = word.replace('_', ' ')\n",
    "        embedding_vector = word2vec_bert(sentence)\n",
    "        # remove start/end markers and take final embedding\n",
    "        embedding_vector = embedding_vector[1:-1, 12]\n",
    "        lengthes[word] = embedding_vector.shape[0]\n",
    "        # average elemntwise. if one embedding vecotr, does not affect the output,\n",
    "        # but if >= 2 embeding vectors (e.g. because of several words in the entity)\n",
    "        # will average elementwise\n",
    "        word_embeddings[word] = torch.mean(embedding_vector, dim=0).cpu().numpy()\n",
    "    return word_embeddings, lengthes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMiAVqbTj9Pn"
   },
   "outputs": [],
   "source": [
    "# Generating embedding takes ~30 min on CPU or ~5-10 min on GPU\n",
    "# Check first if embedding file is available in pkl or .txt\n",
    "# You can jub directly to 'Load file directly instead if generating the new one (if available)'\n",
    "# Get word embeddings with BERT defaul dimmension 768\n",
    "# %time word_set_embeddings, lengthes = get_embeddings(path_dir_wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZWsOFkd20wl"
   },
   "outputs": [],
   "source": [
    "# Save to pickle\n",
    "# with open('./bert_embed_np.pkl', 'wb') as f:\n",
    "    # pkl.dump(word_set_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uyim-zH2i_S"
   },
   "outputs": [],
   "source": [
    "# Save to .txt\n",
    "# write_file('./bert_embed.txt', word_set_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9MpWk77hI0a"
   },
   "source": [
    "## Load file directly instead if generating the new one (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlw589wTgunt"
   },
   "outputs": [],
   "source": [
    "# If saved file is available read it directly\n",
    "with open('./bert_embed_np.pkl', 'rb') as f:\n",
    "    word_set_embeddings = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wyetPvIyaVe"
   },
   "outputs": [],
   "source": [
    "# Plot histogram of embedding vector numbers per node\n",
    "# Note: all embeddings were averaged to get a single embedding per node\n",
    "# Data below for analysis pourpose only\n",
    "plt.hist(lengthes.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y_QRH5CYyNn"
   },
   "outputs": [],
   "source": [
    "# Define below new embedding dimension\n",
    "pca = PCA(n_components = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIJrxSkjY6Oe"
   },
   "outputs": [],
   "source": [
    "word_set_embeddings_only = [v for k, v in word_set_embeddings.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chFDffsgY3DC"
   },
   "outputs": [],
   "source": [
    "pca.fit(word_set_embeddings_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYrVK6qhZpAq"
   },
   "outputs": [],
   "source": [
    "word_set_embeddings_only_transformed = pca.transform(word_set_embeddings_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJsrmu2TfJJz"
   },
   "outputs": [],
   "source": [
    "# Create final dictionary with word: embedding pairs\n",
    "word_set_embeddings_transformed = {k: v for k, v in zip(word_set_embeddings.keys(), word_set_embeddings_only_transformed)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05HLG1qWipyo"
   },
   "outputs": [],
   "source": [
    "# Save to pickle\n",
    "with open('./bert_embed_np_128.pkl', 'wb') as f:\n",
    "    pkl.dump(word_set_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkkpoCKNfIB6"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_tokenizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
