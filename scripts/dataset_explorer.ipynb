{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK WORKIND DIR PATH\n",
    "import os\n",
    "# MODIFY SYSTEM PATH\n",
    "os.chdir('/Users/igorgarbuz/Dev/catlearn')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT BELOW TO SHOW WARNING\n",
    "# import warnings\n",
    "# Needed to show warnings in all Jupyter distributions (e.g. VS Code Jupyter implementation)\n",
    "# warnings.simplefilter(action=\"default\")\n",
    "# pympler is used for recursive sizeof on datastructures\n",
    "from pympler.asizeof import asizeof\n",
    "import random\n",
    "import numpy as np\n",
    "import pprint\n",
    "import itertools\n",
    "import collections\n",
    "import graphistry\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from statistics import mean\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "from catlearn.data.dataset import Dataset\n",
    "from catlearn.graph_utils import (DirectedGraph, clean_selfloops, augment_graph, create_revers_rels)\n",
    "from catlearn.data.utils import write_file, read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For graph plotting login and create a plotter\n",
    "graphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\", username=\"arreason-labs\", password=\"F49owzNdQHbcEU3p\")\n",
    "plotter = graphistry.bind(source='src', destination='dst', node='nodeid', edge_label='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format a graph for plotting\n",
    "def format_for_drawing(G: DirectedGraph, id2entity: dict, id2relation: dict) -> DirectedGraph:\n",
    "    tmp_rep = list(G.edges(data=True))\n",
    "    tmp_rep = [(id2entity[src], id2entity[dst], {'label': id2relation[list(rel.keys())[0]]}) for src, dst, rel in tmp_rep]\n",
    "    return DirectedGraph(tmp_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›  Plotting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_donut(labels, sizes, figsize=(8, 8), title=''):\n",
    "    plt.figure(num=None, figsize=figsize, dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=False)\n",
    "    central_circle = plt.Circle((0,0), 0.75, color='black', fc='white', linewidth=1.25)\n",
    "    fig=plt.gcf()\n",
    "    fig.gca().add_artist(central_circle)\n",
    "    plt.axis('equal')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bars(values: list, height: list, y_log=False, figsize=(25, 3), title: str='', xtick_rotation=0):\n",
    "    \"\"\"Bar plot of list unique numeric values.\n",
    "    values: list[Union(int, float)]\n",
    "    \"\"\"\n",
    "    plt.figure(num=None, figsize=figsize, dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.bar(x=values, height=height)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=xtick_rotation)\n",
    "    if y_log:\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bars_uniques(values: list, y_log=False, figsize=(25, 3), title: str=''):\n",
    "    \"\"\"Bar plot of list unique numeric values.\n",
    "    Made using histogram plotter insted of pyplot.bar.\n",
    "    Has advantage over histogram if data are very sparce\n",
    "    (e.g. significant distance between bins on numerical scale.)\n",
    "    Also has advantage over logarithmic X-axis plot, as log scale cannot always compensate\n",
    "    and must be tuned for sparsity.\n",
    "    values: list[Union(int, float)]\n",
    "    \"\"\"\n",
    "    x = [str(v) for v in values][::-1]\n",
    "    n_bins = len(set(values))\n",
    "    bin_edges = np.arange(n_bins + 1) - 0.5\n",
    "    plt.figure(num=None, figsize=figsize, dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.hist(x=x, bins=bin_edges, align='mid')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Number')\n",
    "    if y_log:\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Number (log)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs statistics\n",
    "1. Show statistics on nodes edges in a matrix for train, valid, test.  \n",
    "2. Show % and number of nodes / edges from valid, test not in train.\n",
    "3. Plot graph total nodes edges, with percentage shared train/valid/test and percentage unique for test/valid.  \n",
    "4. Compute clique index, diameter, centrality, degree histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alysis of previous results\n",
    "\n",
    "1. Any directed graph is updated to have equal number of direct and oposed edges.  \n",
    "2. From (1) any weakly connected graph is equivalent to strongly connected graph.  \n",
    "3. Longest path is equal to maximal order of connection. Longest path is an NP-hard problem.  \n",
    "4. Longest path L is alway < than (N - 1), where N is number of vertices.  \n",
    "5. In any strongly connected graph, by edges composition, longest path is equal to (N - 1).  \n",
    "6. For (5) to be true, maximal order of composition is equal to (N-2) for worst case : star graph for any pair of vertexes.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’¿ Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example WN18 dataset\n",
    "ds_path = './Datasets/wn18rr/text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore WN18 dataset\n",
    "ds = Dataset(path=ds_path, ds_name='wn18', node_vec_dim=10); type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into RAM\n",
    "ds_train = list(ds.train)\n",
    "ds_valid = list(ds.valid)\n",
    "ds_test = list(ds.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_size = asizeof(ds_train)\n",
    "ds_valid_size = asizeof(ds_valid)\n",
    "ds_test_size = asizeof(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of train_ds: {ds_train_size/10**6:.2f} Mb')\n",
    "print(f'Size of valid_ds: {ds_valid_size/10**6:.2f} Mb')\n",
    "print(f'Size of test_ds:  {ds_test_size/10**6:.2f} Mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Total unique entities in Train+Valid+Test:\":50} {len(ds.entity2id)}')\n",
    "print(f'{\"Total unique relations in Train+Valid+Test:\":50} {len(ds.relation2id)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities separatelly per train/valid/test \n",
    "entities_train = [v for tpl in ds_train for v in tpl if not isinstance(v, dict)]\n",
    "entities_valid = [v for tpl in ds_valid for v in tpl if not isinstance(v, dict)]\n",
    "entities_test = [v for tpl in ds_test for v in tpl if not isinstance(v, dict)]\n",
    "# Get uniques\n",
    "entities_train_unique = set(entities_train)\n",
    "entities_valid_unique = set(entities_valid)\n",
    "entities_test_unique = set(entities_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before extracting relations check graph is NOT multirelational\n",
    "max_relations_per_edge_train = max([len(list(v.keys())) for tpl in ds_train for v in tpl if isinstance(v, dict)])\n",
    "max_relations_per_edge_valid = max([len(list(v.keys())) for tpl in ds_valid for v in tpl if isinstance(v, dict)])\n",
    "max_relations_per_edge_test = max([len(list(v.keys())) for tpl in ds_test for v in tpl if isinstance(v, dict)])\n",
    "multirelational = True\n",
    "if (max_relations_per_edge_train == 1\n",
    "    and max_relations_per_edge_valid == 1\n",
    "    and max_relations_per_edge_test == 1\n",
    "):\n",
    "    print(f'Graph is NOT multirelational.')\n",
    "    multirelational = False\n",
    "else:\n",
    "    print('Graph is multirelational.')\n",
    "    print('Check further which part of dataset has multiple relations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relations separatelly per train/valid/test\n",
    "relations_train = (list(v.keys()) for tpl in ds_train for v in tpl if isinstance(v, dict))\n",
    "relations_valid = (list(v.keys()) for tpl in ds_valid for v in tpl if isinstance(v, dict))\n",
    "relations_test = (list(v.keys()) for tpl in ds_test for v in tpl if isinstance(v, dict))\n",
    "if not multirelational:\n",
    "    relations_train = [v[0] for v in relations_train]\n",
    "    relations_valid = [v[0] for v in relations_valid]\n",
    "    relations_test = [v[0] for v in relations_test]\n",
    "else:\n",
    "    relations_train = [v for lst in relations_train for v in lst]\n",
    "    relations_valid = [v for lst in relations_valid for v in lst]\n",
    "    relations_test = [v for lst in relations_test for v in lst]\n",
    "relations_train_unique = set(relations_train)\n",
    "relations_valid_unique = set(relations_valid)\n",
    "relations_test_unique = set(relations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniques relations -- equivalent to graph length\n",
    "print(f'{\"Entities Unique total in Train dataset:\":50}{len(entities_train_unique):10}')\n",
    "print(f'{\"Entities Unique total in Valid dataset:\":50}{len(entities_valid_unique):10}')\n",
    "print(f'{\"Entities Unique total in Test dataset:\":50}{len(entities_test_unique):10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Relations total in Train dataset:\":50}{len(relations_train):10}')\n",
    "print(f'{\"Relations total in Valid dataset:\":50}{len(relations_valid):10}')\n",
    "print(f'{\"Relations total in Test dataset:\":50}{ len(relations_test):10}')\n",
    "# Uniques\n",
    "print()\n",
    "assert relations_test_unique == relations_train_unique == relations_valid_unique\n",
    "print(f'{\"Relations Unique total in train dataset:\":50}{len(relations_train_unique):10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Brancing factor https://en.wikipedia.org/wiki/Branching_factor\n",
    "# NOTE Compute std and/or histogram\n",
    "print(f'{\"Average Branching factor Train:\":40} {len(relations_train)/len(entities_train_unique):<05.3}')\n",
    "print(f'{\"Average Branching factor Valid:\":40} {len(relations_valid)/len(entities_valid_unique):<05.3}')\n",
    "print(f'{\"Average Branching factor Test:\":40} {len(relations_test)/len(entities_test_unique):<05.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check entities in valid and test not in train set\n",
    "valid_not_in_train = []\n",
    "test_not_in_train = []\n",
    "test_not_in_valid = []\n",
    "valid_not_in_test = []\n",
    "def not_in_ds(src: set, ds: set, buffer: list):\n",
    "    for e in src:\n",
    "        if not e in ds:\n",
    "            buffer.append(e)\n",
    "not_in_ds(src=entities_valid_unique, ds=entities_train_unique, buffer=valid_not_in_train)\n",
    "not_in_ds(src=entities_test_unique, ds=entities_train_unique, buffer=test_not_in_train)\n",
    "not_in_ds(src=entities_test_unique, ds=entities_valid_unique, buffer=test_not_in_valid)\n",
    "not_in_ds(src=entities_valid_unique, ds=entities_test_unique, buffer=valid_not_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Entities in Valid and not in Train set:\":50} {len(valid_not_in_train):5}')\n",
    "print(f'{\"Entities in Test and not in Train set:\":50} {len(test_not_in_train):5}')\n",
    "print(f'{\"Entities in test and not in Valid set:\":50} {len(test_not_in_valid):5}')\n",
    "print(f'{\"Entities in Valid and not in Test set:\":50} {len(valid_not_in_test):5}')\n",
    "print()\n",
    "print('In percent:')\n",
    "print(f'{\"Perc. entities in Valid and not in Train set:\":50} {len(valid_not_in_train)*100/len(entities_train_unique):5.3} %')\n",
    "print(f'{\"Perc. entities in Test and not in Train set:\":50} {len(test_not_in_train)*100/len(entities_train_unique):5.3} %')\n",
    "print(f'{\"Perc. entities in Test and not in Valid set:\":50} {len(test_not_in_valid)*100/len(entities_test_unique):5.3} %')\n",
    "print(f'{\"Perc. entities in Valid and not in Test set:\":50} {len(valid_not_in_test)*100/len(entities_valid_unique):5.3} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most of nodes in validation and test sets are present in training set (>99%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example entities\n",
    "start = random.randint(0, len(ds.entity2id))\n",
    "slice_length = 5\n",
    "list(ds.entity2id.items())[start : start + slice_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§± Create directed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train = DirectedGraph(ds_train)\n",
    "graph_valid = DirectedGraph(ds_valid)\n",
    "graph_test = DirectedGraph(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_size = asizeof(graph_train)\n",
    "graph_valid_size = asizeof(graph_valid)\n",
    "graph_test_size = asizeof(graph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Graph size Train: {graph_train_size/10**6:.2f} Mb')\n",
    "print(f'Graph size Valid: {graph_valid_size/10**6:.2f} Mb')\n",
    "print(f'Graph size Test: {graph_test_size/10**6:.2f} Mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Graph length (N Nodes in) Train:\":40}{len(graph_train)}')\n",
    "print(f'{\"Graph length (N Nodes in) Valid:\":40}{len(graph_valid)}')\n",
    "print(f'{\"Graph length (N Nodes in) Test:\":40}{len(graph_test)}')\n",
    "print()\n",
    "print(f'{\"Edges in Train:\":40}{len(graph_train.edges)}')\n",
    "print(f'{\"Edges in Valid:\":40}{len(graph_valid.edges)}')\n",
    "print(f'{\"Edges in Test:\":40}{len(graph_test.edges)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¼ Clean self-loops (edge has the same node at both ends) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_selfloops(graph_train)\n",
    "clean_selfloops(graph_valid)\n",
    "clean_selfloops(graph_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As expected, graph length is equal to number of unique nodes, computed earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter way to get graph statistics.\n",
    "# In/Out degre is equivalent to branching factor computed earlier -- average number of edges per node\n",
    "print(nx.info(graph_train))\n",
    "# print(nx.info(graph_valid))\n",
    "# print(nx.info(graph_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertex statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's plot degree distribution per node\n",
    "Degree of a node -- number of edges per node \n",
    "In addition, check how many nodes in train are connected to the same nodes in valid and test (basically check edges cross-talk).\n",
    "Do cross-talk w/o labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_degrees_train = sorted([len(graph_train[node]) for node in graph_train], reverse=True)\n",
    "vertex_degrees_valid = sorted([len(graph_valid[node]) for node in graph_valid], reverse=True)\n",
    "vertex_degrees_test = sorted([len(graph_test[node]) for node in graph_test], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_first = 0\n",
    "slice_last = -1\n",
    "bins=max(vertex_degrees_train[slice_first:slice_last])\n",
    "plt.figure(num=None, figsize=(25, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.hist(vertex_degrees_train[slice_first:slice_last], bins=bins, rwidth=0.8)\n",
    "plt.title(f'Train vertices degrees distribution over {bins} bins')\n",
    "plt.xlabel('Degree (log)')\n",
    "plt.ylabel('Number (log)')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that histogram does not give a good overview, especially for all range of values.  \n",
    "We have to either slice the list to cut off first ~10 largest, or to try to plot in log space.  \n",
    "Current plot is in log in both, x and y.  Try to comment out x or y log to observe differences in real scales.  \n",
    "  \n",
    "Even after tuning the log space, it doesn't give the granularity necessary to understand the data.  \n",
    "Alternative would be to count the unique values and plot the distribution without intermediate values that are absent.  \n",
    "To remind, histogram is a distribution over a continuous range. A simple 'trick' to compute it over discrete bins would be\n",
    "to convert list of ints to list of strs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of degrees distribution per node\n",
    "# NOTE It's not a histogram as bins are calculated over discret valus\n",
    "vertex_degrees_train_str = [str(v) for v in vertex_degrees_train][::-1]\n",
    "n_bins = len(set(vertex_degrees_train_str))\n",
    "bin_edges = np.arange(n_bins + 1) - 0.5\n",
    "plt.figure(num=None, figsize=(25, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.hist(x=vertex_degrees_train_str, bins=bin_edges, align='mid')\n",
    "plt.title(f'Train vertices degrees distribution over {n_bins} bins')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number (log)')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but with right x-axis scale\n",
    "vertex_degrees_dist = collections.Counter(vertex_degrees_train)\n",
    "x = vertex_degrees_dist.keys()\n",
    "y = vertex_degrees_dist.values()\n",
    "plt.figure(num=None, figsize=(25, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(x=x, height=y, align='center')\n",
    "plt.title(f'Train vertices degrees distribution over {len(x)} bins')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number (log)')\n",
    "plt.yscale('log')\n",
    "plt.xticks(list(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant quantity of nodes are disconnected from the graph. Their re-connection is only possible if there is some semantic information in the words.  \n",
    "\n",
    "Curiously, in training set there are super connencted nodes with 400+ and 200+ connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super_connected_nodes = {ds.id2entity[node] : len(graph_train[node]) for node in graph_train if len(graph_train[node]) > 50}\n",
    "min_n_out_edges = 50\n",
    "nodes_super_out_connctions = ((ds.id2entity[node], len(graph_train[node])) for node in graph_train if len(graph_train[node]) > min_n_out_edges)\n",
    "nodes_super_out_connctions = sorted(nodes_super_out_connctions, key=lambda item: item[1], reverse=True)\n",
    "print(f'Super connected nodes:')\n",
    "for k, v in nodes_super_out_connctions:\n",
    "    print(f'{k:30} {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super_connected_nodes = {ds.id2entity[node] : len(graph_train[node]) for node in graph_train if len(graph_train[node]) > 50}\n",
    "min_n_out_edges = 50\n",
    "nodes_super_out_connctions = ((ds.id2entity[node], len(graph_train[node])) for node in graph_train if len(graph_train[node]) > min_n_out_edges)\n",
    "nodes_super_out_connctions = sorted(nodes_super_out_connctions, key=lambda item: item[1], reverse=True)\n",
    "print(f'Super connected nodes:')\n",
    "for k, v in nodes_super_out_connctions:\n",
    "    print(f'{k:30} {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_degrees_dist = collections.Counter(vertex_degrees_test)\n",
    "x = vertex_degrees_dist.keys()\n",
    "y = vertex_degrees_dist.values()\n",
    "plt.figure(num=None, figsize=(25, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.bar(x=x, height=y, align='center')\n",
    "plt.title(f'Test: vertices degrees distribution over {len(x)} bins (X continuous)')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number (log)')\n",
    "plt.yscale('log')\n",
    "plt.xticks(list(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's represent the graph above as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_degrees(vertex_list: list) -> list:\n",
    "    return dict(sorted(collections.Counter(vertex_list).items(), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set \n",
    "uniques_degrees_count_train = counter_degrees(vertex_degrees_train)\n",
    "uniques_degrees_count_valid = counter_degrees(vertex_degrees_valid)\n",
    "uniques_degrees_count_test = counter_degrees(vertex_degrees_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Uncoment to print out degree count\n",
    "dct = None\n",
    "# dct = uniques_degrees_count_train\n",
    "# dct = uniques_degrees_count_valid\n",
    "dct = uniques_degrees_count_test\n",
    "print('deg, number')\n",
    "print('-----------')\n",
    "pprint.pprint(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that sum of all connections (degrees) count is equal to graph's length\n",
    "assert len(graph_train) == sum(uniques_degrees_count_train.values())\n",
    "assert len(graph_valid) == sum(uniques_degrees_count_valid.values())\n",
    "assert len(graph_test) == sum(uniques_degrees_count_test.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_nodes_no_connection_train = uniques_degrees_count_train[0] * 100 / len(graph_train)\n",
    "prc_nodes_no_connection_valid = uniques_degrees_count_valid[0] * 100 / len(graph_valid)\n",
    "prc_nodes_no_connection_test = uniques_degrees_count_test[0] * 100 / len(graph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_nodes_one_connection_train = uniques_degrees_count_train[1] * 100 / len(graph_train)\n",
    "prc_nodes_one_connection_valid = uniques_degrees_count_valid[1] * 100 / len(graph_valid)\n",
    "prc_nodes_one_connection_test = uniques_degrees_count_test[1] * 100 / len(graph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_nodes_two_connections_train = uniques_degrees_count_train[2] * 100 / len(graph_train)\n",
    "prc_nodes_two_connections_valid = uniques_degrees_count_valid[2] * 100 / len(graph_valid)\n",
    "prc_nodes_two_connections_test = uniques_degrees_count_test[2] * 100 / len(graph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_nodes_more_than_two_connections_train = 100 - prc_nodes_no_connection_train - prc_nodes_one_connection_train - prc_nodes_two_connections_train\n",
    "prc_nodes_more_than_two_connections_valid =  100 - prc_nodes_no_connection_valid - prc_nodes_one_connection_valid - prc_nodes_two_connections_valid\n",
    "prc_nodes_more_than_two_connections_test =  100 - prc_nodes_no_connection_test - prc_nodes_one_connection_test - prc_nodes_two_connections_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col_width = 42\n",
    "print(f'{\"Prc of nodes without connection Train:\":{first_col_width}} {prc_nodes_no_connection_train:.3} %')\n",
    "print(f'{\"Prc of nodes without connection Valid:\":{first_col_width}} {prc_nodes_no_connection_valid:.3} %')\n",
    "print(f'{\"Prc of nodes without connection Test:\":{first_col_width}} {prc_nodes_no_connection_test:.3} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be concluded that Test and Validation sets are very sparse, with almost half of all vertexes been not connected.  \n",
    "On the other hand, train dataset is well connected, with ~70% of all nodes having 1 or 2 Out connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col_width = 42\n",
    "print(f'{\"Prc of nodes one connection Train:\":{first_col_width}} {prc_nodes_one_connection_train:.3} %')\n",
    "print(f'{\"Prc of nodes one connection Valid:\":{first_col_width}} {prc_nodes_one_connection_valid:.3} %')\n",
    "print(f'{\"Prc of nodes one connection Test:\":{first_col_width}} {prc_nodes_one_connection_test:.3} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col_width = 42\n",
    "print(f'{\"Prc of nodes two connections Train:\":{first_col_width}} {prc_nodes_two_connections_train:.3} %')\n",
    "print(f'{\"Prc of nodes two connections Valid:\":{first_col_width}} {prc_nodes_two_connections_valid:.3} %')\n",
    "print(f'{\"Prc of nodes two connections Test:\":{first_col_width}} {prc_nodes_two_connections_test:.3} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col_width = 52\n",
    "print(f'{\"Prc of nodes more than two connections Train:\":{first_col_width}} {prc_nodes_more_than_two_connections_train:.3} %')\n",
    "print(f'{\"Prc of nodes more than two connections Test:\":{first_col_width}} {prc_nodes_more_than_two_connections_valid:.3} %')\n",
    "print(f'{\"Prc of nodes more than two connections Valid:\":{first_col_width}} {prc_nodes_more_than_two_connections_test:.3} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Zero', 'One', 'Two', 'More']\n",
    "sizes = [prc_nodes_no_connection_train, prc_nodes_one_connection_train, prc_nodes_two_connections_train, prc_nodes_more_than_two_connections_train]\n",
    "explode=(0,0,0,0)\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral']\n",
    "plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False)\n",
    "central_circle = plt.Circle((0,0), 0.75, color='black', fc='white', linewidth=1.25)\n",
    "fig=plt.gcf()\n",
    "fig.gca().add_artist(central_circle)\n",
    "plt.axis('equal')\n",
    "plt.title(f'Train vertex degrees distributino', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Zero', 'One', 'Two', 'More']\n",
    "sizes = [prc_nodes_no_connection_valid, prc_nodes_one_connection_valid, prc_nodes_two_connections_valid, prc_nodes_more_than_two_connections_valid]\n",
    "explode=(0,0,0,0)\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral']\n",
    "plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False)\n",
    "central_circle = plt.Circle((0,0), 0.75, color='black', fc='white', linewidth=1.25)\n",
    "fig=plt.gcf()\n",
    "fig.gca().add_artist(central_circle)\n",
    "plt.axis('equal')\n",
    "plt.title(f'Valid vertex degrees distributino', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Zero', 'One', 'Two', 'More']\n",
    "sizes = [prc_nodes_no_connection_test, prc_nodes_one_connection_test, prc_nodes_two_connections_test, prc_nodes_more_than_two_connections_test]\n",
    "explode=(0,0,0,0)\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral']\n",
    "plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False)\n",
    "central_circle = plt.Circle((0,0), 0.75, color='black', fc='white', linewidth=1.25)\n",
    "fig=plt.gcf()\n",
    "fig.gca().add_artist(central_circle)\n",
    "plt.axis('equal')\n",
    "plt.title(f'Test vertex degrees distributino', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relations statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_train = [list(rel.keys())[0] for _, _, rel in ds_train]\n",
    "relations_train_count = collections.Counter(relations_train)\n",
    "tmp = {ds.id2relation[key]: val for key, val in relations_train_count.items()}\n",
    "relations_train_count_str = dict(sorted(tmp.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(values=relations_train_count_str.keys(), height=relations_train_count_str.values(), y_log=False, figsize=(12, 4), title='Relations distin Train set', xtick_rotation=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_donut(labels=relations_train_count_str.keys(), sizes=relations_train_count_str.values(), figsize=(8,8), title='Relations dist in Train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_valid = [list(rel.keys())[0] for _, _, rel in ds_valid]\n",
    "relations_valid_count = collections.Counter(relations_valid)\n",
    "tmp = {ds.id2relation[key]: val for key, val in relations_valid_count.items()}\n",
    "relations_valid_count_str = dict(sorted(tmp.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(values=relations_valid_count_str.keys(), height=relations_valid_count_str.values(), y_log=False, figsize=(12, 4), title='Relations distin Valid set', xtick_rotation=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_test = [list(rel.keys())[0] for _, _, rel in ds_test]\n",
    "relations_test_count = collections.Counter(relations_test)\n",
    "tmp = {ds.id2relation[key]: val for key, val in relations_test_count.items()}\n",
    "relations_test_count_str = dict(sorted(tmp.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars(values=relations_test_count_str.keys(), height=relations_test_count_str.values(), y_log=False, figsize=(12, 4), title='Relations distin Test set', xtick_rotation=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relations sorted by frequency   \n",
    "'_hypernym': 34796,  \n",
    "'_derivationally_related_form': 29715,  \n",
    "'_member_meronym': 7402,  \n",
    "'_has_part': 4816,  \n",
    "'_synset_domain_topic_of': 3116,  \n",
    "'_instance_hypernym': 2921,  \n",
    "'_also_see': 1299,  \n",
    "'_verb_group': 1138,  \n",
    "'_member_of_domain_region': 923,  \n",
    "'_member_of_domain_usage': 629,  \n",
    "'_similar_to': 80  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Betweenness centrality\n",
    "Number (fraction of all-pairs shortest paths) of shortest paths through each node  \n",
    "https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to nx.betweenness_centrality() with multithreading\n",
    "def chunks(l, n):\n",
    "    \"\"\"Divide a list of nodes `l` in `n` chunks\"\"\"\n",
    "    l_c = iter(l)\n",
    "    while 1:\n",
    "        x = tuple(itertools.islice(l_c, n))\n",
    "        if not x:\n",
    "            return\n",
    "        yield x\n",
    "\n",
    "def betweenness_centrality_parallel(G, processes=os.cpu_count()):\n",
    "    \"\"\"Parallel betweenness centrality  function\"\"\"\n",
    "    p = Pool(processes=processes)\n",
    "    node_divisor = len(p._pool) * 4\n",
    "    node_chunks = list(chunks(G.nodes(), int(G.order() / node_divisor)))\n",
    "    num_chunks = len(node_chunks)\n",
    "    bt_sc = p.starmap(\n",
    "        nx.betweenness_centrality_subset,\n",
    "        zip(\n",
    "            [G] * num_chunks,\n",
    "            node_chunks,\n",
    "            [list(G)] * num_chunks,\n",
    "            [True] * num_chunks,\n",
    "            [None] * num_chunks,\n",
    "        ),\n",
    "    )\n",
    "    # Reduce the partial solutions\n",
    "    bt_c = bt_sc[0]\n",
    "    for bt in bt_sc[1:]:\n",
    "        for n in bt:\n",
    "            bt_c[n] += bt[n]\n",
    "    return bt_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If betweenness alraedy computed load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_path = Path(os.getcwd())/'Datasets'/'wn18rr'/'stats'\n",
    "if not os.path.isdir(stats_path):\n",
    "    os.mkdir(stats_path)\n",
    "btwnss_files = [\n",
    "    stats_path/'betweenness_train.txt',\n",
    "    stats_path/'betweenness_valid.txt',\n",
    "    stats_path/'betweenness_test.txt'\n",
    "]\n",
    "# Read files if present in the directory\n",
    "bt_files = [None] * 3\n",
    "bt_train, bt_valid, bt_test = None, None, None\n",
    "for idx, file in enumerate(btwnss_files):\n",
    "    if os.path.isfile(file):\n",
    "        bt_files[idx] = (line.split('\\t') for line in read_file(file))\n",
    "        bt_files[idx] = [(name, float(value)) for name, value in bt_files[idx]]\n",
    "bt_train, bt_valid, bt_test = bt_files\n",
    "bt_train_text, bt_valid_text, bt_test_text = bt_train, bt_valid, bt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute betweenness centrality -- number of shortest paths through each node: https://en.wikipedia.org/wiki/Betweenness_centrality\n",
    "# NOTE for graph_train time >60 min. Uncoment if dispose enough time / resources\n",
    "if not bt_train:\n",
    "    %time bt_train = betweenness_centrality_parallel(graph_train)\n",
    "    bt_train_text = ((ds.id2entity[k], v) for k, v in bt_train.items())\n",
    "    bt_train_text = sorted(bt_train_text, key=lambda tpl: tpl[1], reverse=True)\n",
    "if not bt_valid:\n",
    "    %time bt_valid = betweenness_centrality_parallel(graph_valid)\n",
    "    bt_valid_text = ((ds.id2entity[k], v) for k, v in bt_valid.items())\n",
    "    bt_valid_text = sorted(bt_valid_text, key=lambda tpl: tpl[1], reverse=True)\n",
    "if not bt_test:\n",
    "    %time bt_test = betweenness_centrality_parallel(graph_test)\n",
    "    bt_test_text = ((ds.id2entity[k], v) for k, v in bt_test.items())\n",
    "    bt_test_text = sorted(bt_test_text, key=lambda tpl: tpl[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save betweennees calculation if doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to store wn18 betweenness calculations\n",
    "stats_path = Path(os.getcwd())/'Datasets'/'wn18rr'/'stats'\n",
    "if not os.path.isdir(stats_path):\n",
    "    os.mkdir(stats_path)\n",
    "write_file(path=btwnss_files[0], to_file=dict(bt_train_text))\n",
    "write_file(path=btwnss_files[1], to_file=dict(bt_valid_text))\n",
    "write_file(path=btwnss_files[2], to_file=dict(bt_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore most central nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Most central in Train set:\\n')\n",
    "n_most_central = 40\n",
    "for k, v in bt_train_text[:n_most_central]:\n",
    "    print(f'{k:30} {v:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Most central in Valid set:\\n')\n",
    "n_most_central = 20\n",
    "for k, v in bt_valid_text[:n_most_central]:\n",
    "    print(f'{k:30} {v:.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Most central in Test set:\\n')\n",
    "n_most_central = 20\n",
    "for k, v in bt_test_text[:n_most_central]:\n",
    "    print(f'{k} : {v:.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_n = 50\n",
    "# bins=10\n",
    "# plt.figure(num=None, figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "# plt.hist(dict(bt_train_text[:top_n]).values(), bins=bins, rwidth=0.8)\n",
    "# plt.title(f'Train ds: Betweenness top {top_n} nodes')\n",
    "# plt.xlabel('Betweenness')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_n = 10\n",
    "# bins=10\n",
    "# plt.figure(num=None, figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "# plt.hist(dict(bt_valid_text[:top_n]).values(), bins=10, rwidth=0.8)\n",
    "# plt.title(f'Valid ds: Betweenness top {top_n} nodes')\n",
    "# plt.xlabel('Betweenness')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_n = 10\n",
    "# bins=10\n",
    "# plt.figure(num=None, figsize=(8, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "# plt.hist(dict(bt_test_text[:top_n]).values(), bins=bins, rwidth=0.8)\n",
    "# plt.title(f'Test ds: Betweenness top {top_n} nodes')\n",
    "# plt.xlabel('Betweenness')\n",
    "# plt.ylabel('Occurances')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Diameter\n",
    "Diameter can only be computed on strongly connected graph (path exist entween u, v for all pairs of nodes).\n",
    "Proposed strategy: divide the graph on strongly-connected subgraphs.\n",
    "Compute the diameter for each subgraph. Consider dataset graph diameter as a maximal diameter of all subgraphs.  \n",
    "Single node is considered a strongly connected graph in networkx.\n",
    "Max diameter shows the maximal possible order of connectivity. It gives the maximal number of edges (relations) to be composed to connect two nodes.  \n",
    "  \n",
    "Diameter cannot be computed for weakly connected graph (no disconnected nodes, but due to directiveness it's not possible to find a path for all pairs of nodes).\n",
    "Diameter can be computed for directed and unidirected graphs. Only directional graph can be weakly connected. Unidirectional graph is always strongly-connected or disconnected.  \n",
    "  \n",
    "It is interesting to re-consider input directional graph as unidirectional. \n",
    "X2 oposite connections may exist (a<=>b), in graph theory for a->b<-c: `a` is Not connected to `c`.\n",
    "Hovewer, for knowledge graphs and reasoning, a and c can be considered as connected because they have a common ancestor.\n",
    "E.g. BMW<-hyponym-Vehicle-Hyponym->Truck. Truck and BMW are connected by e.g <-Related-> or <-similar_to-> relation.  \n",
    "Any directional knowledge/logical graph can be augmented with oposit relations for all nodes. Thus it becomes equivalent to unidirectional to compute diameter, shortest path, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert each dataset to a list of subgraphs, either weakly or strongly connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# weakly connected components return a directed graph of minimal length 2\n",
    "# strongly connected components returns a directed graph of minimal length 1 (single node is considered a graph)\n",
    "%time sub_graphs_weakly_train = sorted([graph_train.subgraph(nodes) for nodes in nx.weakly_connected_components(graph_train)], key=lambda x: len(x), reverse=True)\n",
    "%time sub_graphs_weakly_valid = sorted([graph_valid.subgraph(nodes) for nodes in nx.weakly_connected_components(graph_valid)], key=lambda x: len(x), reverse=True)\n",
    "%time sub_graphs_weakly_test = sorted([graph_test.subgraph(nodes) for nodes in nx.weakly_connected_components(graph_test)], key=lambda x: len(x), reverse=True)\n",
    "%time sub_graphs_strongly_train = sorted([graph_train.subgraph(nodes) for nodes in nx.strongly_connected_components(graph_train)], key=lambda x: len(x), reverse=True)\n",
    "%time sub_graphs_strongly_valid = sorted([graph_valid.subgraph(nodes) for nodes in nx.strongly_connected_components(graph_valid)], key=lambda x: len(x), reverse=True)\n",
    "%time sub_graphs_strongly_test = sorted([graph_test.subgraph(nodes) for nodes in nx.strongly_connected_components(graph_test)], key=lambda x: len(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Sorting of lengths here is not necessary, as graphs have already been sorted by length. It's kept for verbosity of keeping lists in decreasing order \n",
    "sub_graphs_weakly_train_lengths = sorted([sub.number_of_nodes() for sub in sub_graphs_weakly_train], reverse=True)\n",
    "sub_graphs_weakly_valid_lengths = sorted([sub.number_of_nodes() for sub in sub_graphs_weakly_valid], reverse=True)\n",
    "sub_graphs_weakly_test_lengths = sorted([sub.number_of_nodes() for sub in sub_graphs_weakly_test], reverse=True)\n",
    "sub_graphs_strongly_train_lengths = sorted([sub.number_of_nodes() for sub in sub_graphs_strongly_train], reverse=True)\n",
    "sub_graphs_strongly_valid_lengths = sorted([sub.number_of_nodes() for sub in sub_graphs_strongly_valid], reverse=True)\n",
    "sub_graphs_strongly_test_lengths = sorted([sub.number_of_nodes() for sub in sub_graphs_strongly_test], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weakly connected subgraph tend to have bigger diameter, but there is much smaller quantity of them\n",
    "# The oposite is observed for strongly connected subgraphs\n",
    "print(f'{\"Number of sub-graphs Weakly Train: \":40}{len(sub_graphs_weakly_train):>5}')\n",
    "print(f'{\"Number of sub-graphs Weakly Valid: \":40}{len(sub_graphs_weakly_valid):>5}')\n",
    "print(f'{\"Number of sub-graphs Weakly Test: \":40}{len(sub_graphs_weakly_test):>5}')\n",
    "print()\n",
    "print(f'{\"Number of sub-graphs Strongly Train: \":40}{len(sub_graphs_strongly_train):>5}')\n",
    "print(f'{\"Number of sub-graphs Strongly Valid: \":40}{len(sub_graphs_strongly_valid):>5}')\n",
    "print(f'{\"Number of sub-graphs Strongly Test: \":40}{len(sub_graphs_strongly_test):>5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For comparaison: total graph lengths below\n",
    "\n",
    "|Name|Value|\n",
    "|:---|---:|\n",
    "|Entities Unique total in Train dataset:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| 40714|  \n",
    "|Entities Unique total in Valid dataset:|  5174|\n",
    "|Entities Unique total in Test dataset:|   5323|  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Min length of Weakly connected graph is:            {min(sub_graphs_weakly_train_lengths)}')\n",
    "print(f'Min length of Strongly connected graph is:          {min(sub_graphs_strongly_train_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sum of all nodes in the Weakly connected subgraph is {sum(sub_graphs_weakly_train_lengths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of all nodes in weakly connected graph is equal to total number of nodes in the graph that means there is no node without a connection.  \n",
    "    \n",
    "We can check it by converting train grain to unidirectional and verifying there is no node without and edge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col_with = 60\n",
    "print(f'{\"Max length of sub-graph Weakly Train: \":{first_col_with}}{max(sub_graphs_weakly_train_lengths):>7}')\n",
    "print(f'{\"Max length of sub-graph Weakly Valid: \":{first_col_with}}{max(sub_graphs_weakly_valid_lengths):>7}')\n",
    "print(f'{\"Max length of sub-graph Weakly Test: \":{first_col_with}}{max(sub_graphs_weakly_test_lengths):>7}')\n",
    "print()\n",
    "print(f'{\"Max length of sub-graph Strongly Train: \":{first_col_with}}{max(sub_graphs_strongly_train_lengths):>7}')\n",
    "print(f'{\"Max length of sub-graph Strongly Valid: \":{first_col_with}}{max(sub_graphs_strongly_valid_lengths):>7}')\n",
    "print(f'{\"Max length of sub-graph Strongly Test: \":{first_col_with}}{max(sub_graphs_strongly_test_lengths):>7}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are ```961``` (~2%) nodes without edges in Train dataset.  \n",
    "All other nodes have at least one connection.  \n",
    "Max length of Weakly connected subgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars_uniques(sub_graphs_weakly_train_lengths, y_log=False, title=f'Train ds: lengths` distribution of {len(sub_graphs_weakly_train_lengths)} Weakly connected sub-graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars_uniques(sub_graphs_weakly_valid_lengths, y_log=True, title=f'Valid ds: lengths` distribution of {len(sub_graphs_weakly_valid_lengths)} Weakly connected sub-graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars_uniques(sub_graphs_weakly_test_lengths, y_log=True, title=f'Test ds: lengths` distribution of {len(sub_graphs_weakly_test_lengths)} Weakly connected sub-graphs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strongly connected subgraphs set contain many length 1 subgraphs -- single node without edges.\n",
    "Particularly for Validation and Test sets. It's expected since a node is a minimal atomic graph component.\n",
    "Given a graph (1->2), it has to strongly-connected subgraphs (1) and (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars_uniques(sub_graphs_strongly_train_lengths, y_log=True, title=f'Train ds: lengths` distribution of {len(sub_graphs_strongly_train_lengths)} Strongly connected sub-graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars_uniques(sub_graphs_strongly_valid_lengths, y_log=True, title=f'Train ds: lengths` distribution of {len(sub_graphs_strongly_valid_lengths)} Strongly connected sub-graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bars_uniques(sub_graphs_strongly_test_lengths, y_log=True, title=f'Train ds: lengths` distribution of {len(sub_graphs_strongly_test_lengths)} Strongly connected sub-graphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There is a large subgraph os strongly-connected nodes 13806 elements. (~33$ of original graph length)')\n",
    "print(f'{sub_graphs_strongly_train_lengths[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot Largest strongly connected subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_path = Path(os.getcwd())/'Datasets'/'wn18rr'/'stats'\n",
    "if not os.path.isdir(stats_path):\n",
    "    os.mkdir(stats_path)\n",
    "diam_file_paths = [\n",
    "    stats_path/'diameter_weakly_train.txt',\n",
    "    stats_path/'diameter_weakly_valid.txt',\n",
    "    stats_path/'diameter_weakly_test.txt',\n",
    "    stats_path/'diameter_strong_train.txt',\n",
    "    stats_path/'diameter_strong_valid.txt',\n",
    "    stats_path/'diameter_strong_test.txt',\n",
    "]\n",
    "# Read files if present in the directory\n",
    "diam_files = [None] * 6\n",
    "diameters_weakly_train, diameters_weakly_valid, diameters_weakly_test = None, None, None\n",
    "diameters_strongly_train, diameters_strongly_valid, diameters_strongly_test = None, None, None\n",
    "for idx, file in enumerate(diam_file_paths):\n",
    "    if os.path.isfile(file):\n",
    "        diam_files[idx] = [int(line) for line in read_file(file)]\n",
    "diameters_weakly_train, diameters_weakly_valid, diameters_weakly_test, diameters_strongly_train, diameters_strongly_valid, diameters_strongly_test = diam_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Diameter can only be computed on either unidirectional graph, or strongly-connected directional graph\n",
    "if not diameters_weakly_train:\n",
    "    %time diameters_weakly_train = [nx.diameter(subg.to_undirected()) for subg in sub_graphs_weakly_train]\n",
    "if not diameters_weakly_valid:\n",
    "    %time diameters_weakly_valid = [nx.diameter(subg.to_undirected()) for subg in sub_graphs_weakly_valid]\n",
    "if not diameters_weakly_test:\n",
    "    %time diameters_weakly_test = [nx.diameter(subg.to_undirected()) for subg in sub_graphs_weakly_test]\n",
    "if not diameters_strongly_train:\n",
    "    %time diameters_strongly_train = [nx.diameter(subg) for subg in sub_graphs_strongly_train]\n",
    "if not diameters_strongly_valid:\n",
    "    %time diameters_strongly_valid = [nx.diameter(subg) for subg in sub_graphs_strongly_valid]\n",
    "if not diameters_strongly_test:\n",
    "    %time diameters_strongly_test = [nx.diameter(subg) for subg in sub_graphs_strongly_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force = True\n",
    "diam_files = [\n",
    "    stats_path/'diameter_weakly_train.txt',\n",
    "    stats_path/'diameter_weakly_valid.txt',\n",
    "    stats_path/'diameter_weakly_test.txt',\n",
    "    stats_path/'diameter_strong_train.txt',\n",
    "    stats_path/'diameter_strong_valid.txt',\n",
    "    stats_path/'diameter_strong_test.txt',\n",
    "]\n",
    "write_file(path=diam_files[0], to_file=diameters_weakly_train, force=force)\n",
    "write_file(path=diam_files[1], to_file=diameters_weakly_valid, force=force)\n",
    "write_file(path=diam_files[2], to_file=diameters_weakly_test, force=force)\n",
    "write_file(path=diam_files[3], to_file=diameters_strongly_train, force=force)\n",
    "write_file(path=diam_files[4], to_file=diameters_strongly_valid, force=force)\n",
    "write_file(path=diam_files[5], to_file=diameters_strongly_test, force=force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run with enough resources\n",
    "# %time diameters_strongly_train = [nx.diameter(subg) for subg in sub_graphs_strongly_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Diameter Weakly Train: \":30}{max(diameters_weakly_train)}')\n",
    "print(f'{\"Diameter Weakly Valid: \":30}{max(diameters_weakly_valid)}')\n",
    "print(f'{\"Diameter Weakly Test: \":30}{max(diameters_weakly_test)}')\n",
    "print()\n",
    "print(f'{\"Diameter Strongly Train: \":30}{max(diameters_strongly_train)}')\n",
    "print(f'{\"Diameter Strongly Valid: \":30}{max(diameters_strongly_valid)}')\n",
    "print(f'{\"Diameter Strongly Test: \":30}{max(diameters_strongly_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Graph|Diameter|\n",
    "|:---|:------:|\n",
    "|Train Weakly|22|\n",
    "|Valid Weakly|10|\n",
    "|Test Weakly|6|\n",
    "|---------------|------|\n",
    "|Train Strongly|40|\n",
    "|Valid Strongly|1|\n",
    "|Test Strongly|2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph diameter is 40 when considered Strongly connected graph (path in both directions for each pair of nodes)\n",
    "### Graph diameter is 22 when considered Weakly connected graph (non-directed connection in at least one direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sub-graph Weakly connected Drawn with native Matplotlib tools\n",
    "nx.draw_networkx(sub_graphs_weakly_train[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sub-graph Strongly connected\n",
    "nx.draw_networkx(sub_graphs_strongly_train[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Compute Clique -- complete subgraphs\n",
    "# Clique = subgraph where all vertex are strongly connected to all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE Compute K neighbors -- compute degree of graph connectiveness\n",
    "# Use k-neighbor connectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export graph to a file to open in 3rd party software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph export for Gephi\n",
    "# NOTE Uncomment to export\n",
    "# ds_train_exp = [(ds.id2entity[src], ds.id2entity[dst], {'label':ds.id2relation[list(rel.keys())[0]]}) for src, dst, rel in ds_train]\n",
    "# graph_train_exp = DirectedGraph(ds_train_exp)\n",
    "# nx.write_gexf(graph_train_exp, 'train.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph export to json (e.g. for D3 js library)\n",
    "# NOTE Uncomment to export\n",
    "# import json\n",
    "# ds_train_exp = [(ds.id2entity[src], ds.id2entity[dst], {'label':ds.id2relation[list(rel.keys())[0]]}) for src, dst, rel in ds_train]\n",
    "# graph_train_exp = DirectedGraph(ds_train_exp)\n",
    "# d3_data = nx.node_link_data(graph_train_exp)\n",
    "# with open('graph_train_d3.json', 'w') as f:\n",
    "#     json.dump(d3_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment dataset (Graph) to have an oposite relation for each relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two possible strategies for dataset augmentation:  \n",
    "1. Double number of relations by creating r' for each r in R such that r' is oposite to r,  \n",
    "2. Manually creating dictionary of oposite relations when each relation semantic value is available.\n",
    "3. Mixed strategy where some relations are takes from the available dictionary, and some are created.  \n",
    "\n",
    "For strategy #1, number of relation labels doubles. OH vector of relations doubles.  \n",
    "For strategy #2, number of relation labels remains the same.  \n",
    "For strategy #3, number of relations is increased to < 2*N, where N is initial number of relations in the dataset.  \n",
    "Problem with strategies 1,2 :  \n",
    "For relations prediction task, if model predict a relation r' (oposite to r), that doesn't in the original dataset,\n",
    "it's prediciton only implies that there is probably a relation r in oposite direction. But the prediction r' cannot be exploited to improve the score.\n",
    "Thus, only strategy 3 can directly improve the model by adding more symantics to the relation labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_revers = {\n",
    "    # sorted by frequency\n",
    "    # asymetric. super<>sub-ordinate\n",
    "    '_hypernym': None, # super-sub-ordinate. inheritble. meta-class. Oposit to hyponym !INVERTED!\n",
    "    # symetric\n",
    "    '_derivationally_related_form': '_derivationally_related_form', # https://stackoverflow.com/questions/39707654/what-are-derivationally-related-forms-in-wordnet\n",
    "    # asymetric. whole<>part relations\n",
    "    # '_member_meronym': '_has_part', # finger is meronym of hand. oposite to holonym (set constituted of heronyms) !INVERTED!\n",
    "    '_member_meronym': None, # finger is meronym of hand. oposite to holonym (set constituted of heronyms) !INVERTED!\n",
    "    # '_has_part': '_member_meronym', # oposite to meronym meronym, hand has_part finger. same as holonym. Since _member_meronym is inverted, cannot be used.\n",
    "    '_has_part': None, # oposite to meronym meronym, hand has_part finger. same as holonym. Since _member_meronym is inverted, cannot be used.\n",
    "    # asymetric. member<>topic\n",
    "    '_synset_domain_topic_of': None, # not symetrical relation. e.g wave _synset_domain_topic_of physics. But physics is not topic of wave. oposite should be a domain.\n",
    "    # asymetric. super<>sub-ordinate\n",
    "    '_instance_hypernym' : None, # meta-class of an instance of something. !INVERTED!\n",
    "    # symetric\n",
    "    '_also_see': '_also_see',\n",
    "    '_verb_group': '_verb_group',\n",
    "    # asymetric. member<>domain\n",
    "    '_member_of_domain_usage': None, # !INVERTED!\n",
    "    '_member_of_domain_region': None, # !INVERTED!\n",
    "    #symetric\n",
    "    '_similar_to': '_similar_to',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation2id_augmented, relation_id2vec_augmented, revers_rels = create_revers_rels(relation_revers, ds.relation2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before augmentation\n",
    "print(nx.info(graph_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_graph(graph_train, revers_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before augmentation\n",
    "print(nx.info(graph_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Draw and explore graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_relations(list_triples: list, relation: str, num = 30):\n",
    "    for src, dst, rel in list_triples:\n",
    "        rel_str = list(rel.values())[0]\n",
    "        if (rel_str == relation):\n",
    "            print(f'{src:20} -- {rel_str:15} -> {dst}')\n",
    "            num -= 1\n",
    "        if not num:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = format_for_drawing(graph, id2entity=ds.id2entity, id2relation=id2relation_augmented_1)\n",
    "plotter.plot(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = format_for_drawing(graph_test)\n",
    "plotter.plot(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = format_for_drawing(graph_valid)\n",
    "plotter.plot(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
